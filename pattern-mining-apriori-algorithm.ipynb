{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03327982",
   "metadata": {},
   "source": [
    "# Note:\n",
    "The entire code took around one hour forty five minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74ad025b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\srijitac\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import all libs here\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "from bson import Code\n",
    "import pymongo\n",
    "import pprint\n",
    "\n",
    "# Import all relevant libraries for pre-processing\n",
    "# Import tokenize for splitting sentences and words\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "import string\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a4e7da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for connection :  0.010938882827758789 s.\n"
     ]
    }
   ],
   "source": [
    "# make connection\n",
    "st = time()\n",
    "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "print(\"Time taken for connection : \", time() - st, \"s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0676a974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to load data :  0.6961395740509033 s.\n"
     ]
    }
   ],
   "source": [
    "# create collection litcovidSample and load data\n",
    "st = time()\n",
    "\n",
    "db = client['covidData2']\n",
    "covidItems = db['test']\n",
    "db.list_collection_names()\n",
    "if 'test' in db.list_collection_names():\n",
    "    db['test'].drop()\n",
    "litcovid = db[\"test\"]\n",
    "\n",
    "with open('litcovid2BioCJSON_small.json') as f:\n",
    "    file_data = json.load(f)\n",
    "    \n",
    "covidItems.insert_many(file_data)\n",
    "\n",
    "print(\"Time taken to load data : \", time() - st, \"s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ce1e1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78952\n",
      "Time taken to preprocess the data :  14.378509283065796 s.\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing of data\n",
    "\n",
    "st = time()\n",
    "\n",
    "# List of punctuations\n",
    "punct = list(string.punctuation)\n",
    "\n",
    "# Extract all the texts from the passages\n",
    "texts_array = []\n",
    "for y in covidItems.find():\n",
    "    for x in y['passages']:\n",
    "        texts_array.append(x['text'])\n",
    "\n",
    "# Split texts_array into individual sentences\n",
    "sen_array = []\n",
    "for a in texts_array:\n",
    "    b = sent_tokenize(a)\n",
    "    sen_array.extend(b)\n",
    "\n",
    "# Split sen_array to individual words\n",
    "# Remove stop words and punctuations and digits\n",
    "words_array = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for a in sen_array:\n",
    "    b = word_tokenize(a.lower())\n",
    "    temp_array = []\n",
    "    \n",
    "    for x in b:        \n",
    "        if (x not in stop_words) and not x.isdigit() and not x.replace('.', '', 1).isdigit() \\\n",
    "        and (x not in punct) and (len(x)>2) and (x not in temp_array):\n",
    "            temp_array.append(x)\n",
    "    if(len(temp_array) > 0 ):\n",
    "        words_array.append(temp_array)\n",
    "\n",
    "print(len(words_array))\n",
    "\n",
    "print(\"Time taken to preprocess the data : \", time() - st, \"s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c970a015",
   "metadata": {},
   "source": [
    "# Frequent Itemset Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a2d9b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00253318471982977"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_support = 200/len(words_array)\n",
    "min_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a2c5543",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_list = list(map(set, words_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb92ab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creation of list of unique words from the entire words_array\n",
    "def list_1_creation(words_array):\n",
    "    \n",
    "    list1 = []\n",
    "    for word in words_array:\n",
    "        for item in word:\n",
    "            if not [item] in list1:\n",
    "                list1.append([item])\n",
    "    list1.sort()\n",
    "    \n",
    "    # create a set for each item in list1\n",
    "    return [set(x) for x in list1]\n",
    "\n",
    "C1 = list_1_creation(words_array)\n",
    "\n",
    "##Creates patterns based on pattern number(1,2,3)\n",
    "##Example of pattern number : if pattern_nbr = 2 then (a,b)(a,c)(b,c)\n",
    "def list_n_creation(list_n, pattern_nbr):\n",
    "    \n",
    "    final_list = []\n",
    "    len_list = len(list_n)\n",
    "    \n",
    "    # join sets if first pattern_nbr-2 items are equal\n",
    "    for i in range(len_list):\n",
    "        for j in range(i+1, len_list):\n",
    "            L1 = list(list_n[i])[:pattern_nbr-2]\n",
    "            L2 = list(list_n[j])[:pattern_nbr-2]\n",
    "            L1.sort()\n",
    "            L2.sort()\n",
    "            if L1==L2:\n",
    "                final_list.append(list_n[i] | list_n[j])\n",
    "    \n",
    "    return final_list\n",
    "\n",
    "##scanD = freq_itemset_calculater\n",
    "## data = set_data\n",
    "## Ck = freq_itemset\n",
    "def freq_itemset_calculater(set_data, freq_itemset, min_support):\n",
    "    \"\"\"\n",
    "    Scan through transaction data and return a list of candidates that meet\n",
    "    the support threshold, and support data about the current candidates.\n",
    "    \n",
    "    Arguments:\n",
    "       set_data: data set,\n",
    "       freq_itemset: a list of candidate sets\n",
    "       min_support: the minimum support\n",
    "    \"\"\"\n",
    "    count = {}\n",
    "    len_d = float(len(unique_list))\n",
    "    ##Create dictionry with word and its frequency\n",
    "    for item in set_data:\n",
    "        for freq_word in freq_itemset:\n",
    "            if freq_word.issubset(item):\n",
    "                fw = frozenset(freq_word)\n",
    "                if fw not in count:\n",
    "                    count[fw] = 1                \n",
    "                else:\n",
    "                    count[fw] += 1\n",
    "                    num_items = len_d\n",
    "\n",
    "    num_items = len_d\n",
    "    final_list = []\n",
    "    support_data = {}\n",
    "    support_data_transposed = {}\n",
    "    \n",
    "    # calculate support for every itemset in count dictionary\n",
    "    for key in count:\n",
    "        support_value = count[key]/num_items      \n",
    "        \n",
    "        # If the support meets the minimum support requirements, add it to the lists.\n",
    "        if support_value >= min_support:\n",
    "            final_list.insert(0, key)\n",
    "            support_data[key] = support_value\n",
    "            support_data_transposed[support_value] = key\n",
    "    \n",
    "    return final_list, support_data, support_data_transposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d87e0eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to create first frequent itemset :  253.4085648059845 s.\n"
     ]
    }
   ],
   "source": [
    "st = time()\n",
    "\n",
    "list1 = list_1_creation(words_array)\n",
    "\n",
    "print(\"Time taken to create first frequent itemset : \", time() - st, \"s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a82814a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to find first frequent itemset :  285.6554272174835 s.\n"
     ]
    }
   ],
   "source": [
    "st = time()\n",
    "\n",
    "L1, support_data1, support_data1_tansposed = freq_itemset_calculater(unique_list, list1, min_support)\n",
    "\n",
    "print(\"Time taken to find first frequent itemset : \", time() - st, \"s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc9d8a79",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to find frequent itemset  2  :  1670.1855034828186 s.\n",
      "Time taken to find frequent itemset  3  :  12.286511182785034 s.\n",
      "Time taken to find frequent itemset  4  :  0.10970878601074219 s.\n",
      "Time taken to find frequent itemset  5  :  0.03989291191101074 s.\n",
      "Time taken to find frequent itemset  6  :  0.01495051383972168 s.\n",
      "Time taken to find frequent itemset  7  :  0.007018566131591797 s.\n"
     ]
    }
   ],
   "source": [
    "L = L1\n",
    "C = C1\n",
    "itr = 2\n",
    "\n",
    "#list of frequent items\n",
    "freq_itemset_lists = []\n",
    "#list of patterns\n",
    "pattern_lists = []\n",
    "#list of frequent items with their support values\n",
    "support_value_lists = []\n",
    "#list of support values with their frequent items\n",
    "support_value_transposed_lists = []\n",
    "#list of unique support values\n",
    "unique_support_lists = []\n",
    "\n",
    "freq_itemset_lists.append(L)\n",
    "pattern_lists.append(C)\n",
    "support_value_lists.append(support_data1)\n",
    "support_value_transposed_lists.append(support_data1_tansposed)\n",
    "\n",
    "for key, item in support_data1.items():\n",
    "    unique_support_lists.append(item)\n",
    "            \n",
    "while len(L) != 0:\n",
    "    \n",
    "    st = time()\n",
    "    \n",
    "    C = list_n_creation(L, itr)\n",
    "    L, support_data, support_data_tansposed = freq_itemset_calculater(unique_list, C, min_support)\n",
    "    \n",
    "    if len(L) != 0:\n",
    "        freq_itemset_lists.append(L)\n",
    "        pattern_lists.append(C)\n",
    "        support_value_lists.append(support_data)\n",
    "        support_value_transposed_lists.append(support_data_tansposed)\n",
    "        for key, item in support_data.items():\n",
    "            unique_support_lists.append(item)\n",
    "            \n",
    "    itr = itr + 1\n",
    "    \n",
    "    print(\"Time taken to find frequent itemset \", itr-1, \" : \", time() - st, \"s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6add4dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_support_set = set(unique_support_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "814bdaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {}\n",
    "for i in support_value_transposed_lists:\n",
    "    \n",
    "    for key, value in i.items():\n",
    "        if key in dictionary:\n",
    "        \n",
    "            if not isinstance(dictionary[key], list):                \n",
    "                dictionary[key] = [dictionary[key]]\n",
    "            \n",
    "            dictionary[key].append(value)\n",
    "        else:\n",
    "            dictionary[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81e9d7c",
   "metadata": {},
   "source": [
    "# Finding Closed Pattern "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1faaddad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL CLOSED PATTERNS : 1144\n",
      "\n",
      "TOP 5 2-ITEMSET CLOSED PATTERNS :\n",
      "\n",
      "Itemset:  ['patients', 'covid-19']\n",
      "Support:  3240\n",
      "\n",
      "Itemset:  ['pandemic', 'covid-19']\n",
      "Support:  1905\n",
      "\n",
      "Itemset:  ['covid-19', 'disease']\n",
      "Support:  1367\n",
      "\n",
      "Itemset:  ['study', 'covid-19']\n",
      "Support:  1087\n",
      "\n",
      "Itemset:  ['coronavirus', 'disease']\n",
      "Support:  1045\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Close patterns and top 5 close patterns \n",
    "\n",
    "close_patterns = []\n",
    "one_close_pattern_sup = []\n",
    "one_close_pattern = []\n",
    "two_close_pattern_sup = []\n",
    "two_close_pattern = []\n",
    "\n",
    "for dicts in support_value_lists:\n",
    "    for itemset, support in dicts.items():\n",
    "        is_close_pattern = True        \n",
    "        f_itemsets = itemset\n",
    "        f_support = support\n",
    "        f_length = len(itemset)\n",
    "        item_sets_to_check = dictionary[f_support]\n",
    "        for it in item_sets_to_check:\n",
    "            if (f_itemsets != it):\n",
    "                if(frozenset.issubset(f_itemsets, it)):\n",
    "                    is_close_pattern = False\n",
    "                    break\n",
    "        if(is_close_pattern):\n",
    "            close_patterns.append(itemset)\n",
    "            if f_length == 1:\n",
    "                one_close_pattern.append(itemset)\n",
    "                one_close_pattern_sup.append(support)\n",
    "            if f_length == 2:\n",
    "                two_close_pattern.append(itemset)\n",
    "                two_close_pattern_sup.append(support)\n",
    "            \n",
    "print(\"TOTAL CLOSED PATTERNS : \" + str(len(close_patterns)))\n",
    "print()\n",
    "\n",
    "print(\"TOP 5 2-ITEMSET CLOSED PATTERNS :\")\n",
    "indexes = sorted(range(len(two_close_pattern_sup)), key=lambda i: two_close_pattern_sup[i])[-5:]\n",
    "indexes.reverse()\n",
    "print()\n",
    "for i in range(0, len(indexes)):\n",
    "    print(\"Itemset: \", str(list(two_close_pattern[indexes[i]])))\n",
    "    print(\"Support: \", str(math.ceil(two_close_pattern_sup[indexes[i]] * len(words_array))))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10582660",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e8e60e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
